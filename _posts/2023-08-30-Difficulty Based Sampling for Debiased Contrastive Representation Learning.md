---
title : Difficulty-Based Sampling for Debiased Contrastive Representation Learning (2023)
tags : 논문리뷰 
---

## Abstract 
대조 학습은 다양한 분류 작업에서 획기적인 성능을 달성하는 자기 지도 표현 학습 방법입니다. 그러나 비지도 방식이기 때문에 다른 레이블을 가진 것으로 가정되지만 실제로는 앵커와 동일한 레이블을 가진 것으로 간주되는 무작위로 추출된 음성 샘플인 위음성 샘플 문제가 발생합니다. 이는 의미적으로 유사한 쌍과 그렇지 않은 쌍을 대조하려는 동기와 모순되기 때문에 대조 학습의 성능을 저하시킵니다. 이로 인해 1) 참 네거티브와 거짓 네거티브, 2) 쉬운 네거티브와 어려운 네거티브를 구분하여 해결해야 하는 합법적인 네거티브 샘플을 찾는 것에 대한 관심과 중요성이 높아졌습니다. 그러나 기존 연구는 하이퍼파라미터 튜닝을 통해 거짓 음성 및 하드 네거티브 샘플을 처리하는 통계적 접근 방식에 국한되어 있었습니다. 이 논문에서는 통계적 접근 방식을 넘어 하드 네거티브 샘플과 데이터 편향 사이의 연관성을 탐구합니다. 우리는 편향 증폭 상대값을 상대적으로 어렵게 참조하여 하드 네거티브를 탐색하는 새로운 비편향 대조 학습 방법을 소개합니다. 쉬운 네거티브 샘플에 더 집중하는 편향된 인코더를 훈련하기 위해 삼중 손실을 제안합니다. 이론적으로 삼중 손실이 자기 지도 표현 학습에서 편향을 증폭시킨다는 것을 보여줍니다. 마지막으로 제안한 방법이 다운스트림 분류 성능을 향상시킨다는 것을 실증적으로 보여줍니다.

## Introduction 
양성 샘플링과 달리, 합법적인 음성 샘플을 찾는 것은 사소한 문제가 아닙니다. 첫째, 부정 샘플은 비지도 대조 학습 방식으로 인해 앵커[8, 19]와 다른 클래스를 갖는다는 보장이 없습니다. 따라서 한계 데이터 분포를 분해하여 이러한 위음성 문제를 해결하기 위해 Debiasing 방법[8]이 제안되었습니다. 둘째, 앵커와 구별하기 어려운 하드 네거티브 샘플을 찾는 것이 더 많은 정보를 제공하기 때문에 중요합니다[36]. 지도 대조 학습[19]은 하드 네거티브 마이닝의 중요성을 검증했습니다. 그러나 이 방법은 문헌에서 거의 연구되지 않았습니다. <br/>
분류와 같은 지도 학습에서 하드 샘플을 관찰한 일부 작업은 데이터 편향과 관련이 있습니다. 일부 모델은 이미지 분류의 질감, 색상, 배경[2,25], 얼굴 인식의 인종 및 성별[17]과 같은 편향 속성과 목표 레이블 간의 상관관계에 의해 오도되는 경향이 있기 때문에 이러한 상관관계에 반하는 샘플은 하드 샘플일 가능성이 높습니다. 예를 들어, 동물 분류 작업에서 훈련 세트의 대부분의 새 이미지가 다른 이미지가 아닌 하늘을 배경으로 하고 있다고 가정하면 하늘은 새라는 클래스와 강한 상관관계가 있을 것입니다. 그러나 새는 물, 바위 등과 같은 다른 배경에도 존재할 수 있습니다. 하늘이 아닌 배경에 있는 새는 편향이 충돌하는 표본으로 간주할 수 있습니다. 그렇다면 편향이 일치하는 샘플(하늘의 새)보다 편향이 충돌하는 샘플(물 위의 새)이 더 많은 정보를 제공하기 때문에 더 나은 성능과 일반화를 위해 편향이 충돌하는 샘플을 더 강조하는 것은 당연합니다. 대조 학습 관점에서 이러한 편향 충돌 샘플은 앵커(예: 물 위의 개구리)와 구별하기 어려울 가능성이 높으며, 자연스럽게 표현 공간에서 하드 네거티브와 연결됩니다.
<br/>
이전 연구와 달리, 우리는 무엇이 자기 지도 학습에서 샘플을 하드 네거티브 또는 이지 네거티브로 만드는가라는 질문을 탐구합니다. 우리가 아는 한, 이러한 관점에서 대조 학습에 대한 연구는 거의 수행되지 않았습니다. 이 연구에서는 데이터 편향의 관점에서 하드 네거티브 샘플을 효과적으로 찾기 위한 새로운 대조 학습 방법을 제안합니다. 우리는 삼중 손실[38]을 사용하여 편향이 증폭된 표현을 자기 지도 방식으로 학습합니다. 5.2절에서는 삼중 손실을 최소화하면 모델이 쉬운 샘플에 집중하고 어려운 샘플은 무시하도록 강제한다는 것을 이론적으로 보여줍니다. 편향된 모델과 함께 두 모델의 표현과 샘플 난이도의 대리인인 앵커 사이의 상대적 거리를 측정하여 각 샘플의 상대적 난이도를 기반으로 편향된 모델을 훈련합니다.<br/>

### contribution
1. 하드 네거티브와 이지 네거티브, 참 네거티브와 거짓 네거티브의 두 가지 유형의 편향성을 해결하는 편향된 대조 학습 방법을 제안합니다. 
2. 자기 감독 대조 학습에서 샘플의 상대적 난이도를 학습하는 데 효과적인 대리자 역할을 하는 삼중 손실을 도입하여 표현 공간의 편향성을 증폭시킵니다. 
3. 학습된 표현이 이미지 및 표 형식 데이터 분류에서 관련 방법과 비교하여 다운스트림 작업에서 더 높은 정확도와 감소된 편향성을 달성한다는 것을 경험적으로 검증합니다.

## Motivation 
대조 학습의 목표에 따라, 우리는 양성 샘플이 앵커에 가깝고 음성 샘플은 앵커와 떨어져 있기를 원합니다. 데이터 편향의 관점에서 볼 때, 편향이 충돌하는 부정 샘플은 다른 클래스의 샘플임에도 불구하고 의미적으로 앵커와 유사합니다. 결과적으로 코사인 유사도는 ||x - y||2 2 ∝ -2x ⊺y로 두 샘플 사이의 거리에 비례하기 때문에 앵커, 즉 하드 네거티브에 가깝게 임베드될 가능성이 높습니다. <br/>
대조 학습의 목표에 따라, 우리는 양성 샘플이 앵커에 가깝고 음성 샘플은 앵커와 떨어져 있기를 원합니다. 데이터 편향의 관점에서 볼 때, 편향이 충돌하는 부정 샘플은 다른 클래스의 샘플임에도 불구하고 의미적으로 앵커와 유사합니다. 결과적으로 코사인 유사도는 ||x - y||2 2 ∝ -2x ⊺y로 두 샘플 사이의 거리에 비례하기 때문에 앵커, 즉 하드 네거티브에 가깝게 임베드될 가능성이 높습니다. 마찬가지로, 편향이 충돌하는 양성 샘플은 각각 앵커, 즉 하드 양성으로부터 멀리 떨어져 있을 가능성이 높습니다. 반대로, 편향이 일치하는 음성(양성) 샘플은 앵커에서 멀리(또는 가깝게) 포함되어 쉽게 구분할 수 있습니다. 다음 문맥에서는 바이어스 상충/정렬과 하드/쉬운을 같은 의미로 사용합니다.
<br/>

* Conflicting = hard : (negative) 다른 클래스에 속하더라도 의미적으로 앵커와 유사, (positive) 같은 클래스에 속하는데 앵커로 부터 멀리 떨어져 잇음   
* Alligned = easy : (negative) 앵커에서 멀리, (positive) 앵커에서 가까워서 쉽게 구분할 수 있음 

편향된 표현 학습을 해결하려면 하드 포지티브/네거티브 샘플에 집중해야 합니다. 하지만 대부분의 경우 양성 샘플은 앵커의 단순한 증강에 불과하므로, 우리는 하드 네거티브 샘플에 집중할 것입니다. 
<br/>
![](/assets/img/2023-08-30-13-31-31.png)
<br/>

이러한 바이어스 증폭 인코더가 있는 경우 두 인코더에서 앵커와 네거티브 샘플 사이의 상대적 거리를 비교하여 네거티브 샘플의 난이도를 추정할 수 있습니다.

## Problem definition 
### Self-Supervised Contrastive Learning 
N : positive pair의 갯수
M : negative pair의 갯수
Contrastive learning은 positive pair끼리는 가깝게, negative pair끼리는 멀게 아래와 같은 수식을 바탕으로 학습함 <br/>
![](/assets/img/2023-08-30-13-38-42.png)
<br/>

### Debiased Contrastive Learning 
효과적인 대조 학습 모델을 훈련하기 위해서는 1) 참음성과 거짓음성, 2) 쉬운 음성과 어려운 음성을 구분하는 것이 중요한데, 이를 위해서는 합법적인 음성 샘플을 찾는 것이 중요합니다. 

### Debias False Negative 
비지도 설정에서 모든 네거티브가 반드시 진정한 네거티브 샘플인 것은 아닙니다. 즉, 앵커와 같은 클래스를 가진 네거티브 샘플이 있을 수 있습니다. 이를 보완하기 위해 한계 데이터 분포 p(x)를 (1)과 같이 분해합니다. <br/>
![Alt text](/assets/img/image.png)<br/>

### Hard Negative Mining 
어려운 음성/쉬운 음성을 효과적으로 구분하는 것은 사소한 문제가 아닙니다. Importance sampling 기법은 어려운 음성 샘플에 초점을 맞추기 위해 제안되었습니다. 
<br/>
그러나 이 중요도 샘플링 기술을 사용하면, 편향 제거를 위해 앵커와 높은 유사성을 가진 긍정적인 샘플, 즉 쉬운 긍정 샘플 또는 편향을 일치시키는 긍정 샘플도 샘플링해야 합니다. 더욱이, (3)의 마지막 항에 있는 $q+$ 의 편향 제거 항은 대조적 손실을 최소화함에 따라 쉬운 긍정 샘플이 앵커로부터 더 멀어지게 만듭니다. 이로 인해 어려운 부정 샘플과 쉬운 긍정 샘플 사이에 트레이드오프가 생기며, 이는 훈련이 불안정해질 위험이 있습니다. 또한, 추가적인 유사도 가중치 하이퍼파라미터 $\beta$가 있어, 이를 조정하기 위한 추가적인 노력이 필요합니다. 

## Difficulty based Debiasing Methods
### Debiased Contrastive Learning with Difficulty based Sampling
기존의 대조 학습 방법과 달리, 우리의 모델은 두 개의 인코더 Ed와 Eb로 구성됩니다. Ed는 편향을 완화할 수 있는 하드 네거티브 샘플을 효과적으로 처리하고, Eb는 쉬운 샘플에만 초점을 맞춰 편향을 의도적으로 증폭하는 역할을 합니다.
<br/>
이전의 identity 분류[38] 및 지도 대조 학습[19]에 대한 지도 학습 작업에서 영감을 받아, 저희는 삼중 손실을 사용하여 Eb의 편향을 증폭시킵니다. 그러나 이전에는 삼중 손실이 감독된 방식으로 사용되었기 때문에 레이블이 없는 환경에서는 적용되지 않습니다. 이 문제를 해결하기 위해 수식(5)와 같이 자체 감독 삼중 손실을 도입했습니다:
<br/>
![](/assets/img/2023-08-30-13-58-26.png)
<br/>

$L_tri$를 최소화하기 위해 gradient descent를 통해 $E_b$를 update하면 아래와 같다. 
<br/>
![](/assets/img/2023-08-30-13-59-33.png)
<br/>
또한 자가 감독 설정에서 삼중 손실을 채택함에 따라 (5)의 두 번째 항을 (2)와 유사하게 오탐을 제거하고 대체합니다:
<br/>
![](/assets/img/2023-08-30-14-06-21.png)
<br/>
또한 여러 인코더의 결과를 간단히 비교하여 상대적인 난이도를 측정하면 까다로운 하이퍼파라미터 튜닝을 피할 수 있습니다.
<br/>
즉, 상대적 난이도 함수 w에 다음과 같이 곱하여 편향성이 있는 샘플에 더 많은 불이익을 줄 수 있습니다:
![](/assets/img/2023-08-30-14-07-32.png)
<br/>
여기서는 네거티브 샘플의 두 임베딩 사이의 거리를 직접 비교하는 대신 앵커를 기준점으로 사용하여 상대적인 난이도를 측정합니다. 이는 잠재 공간이 반지름이 1/t인 하이퍼스피어이므로 두 임베딩이 모두 앵커 포인트와 유사하더라도 거리가 멀 수 있기 때문입니다. (1)과 같이 τ +가 미리 주어지거나 이를 하이퍼파라미터로 사용하면 잘못 선택된 음수 샘플을 더욱 완화할 수 있으며 최종 손실은 아래와 같습니다. 
<br/>
![](/assets/img/2023-08-30-14-08-36.png)
<br/>
제안된 표현 학습 방법의 개요는 그림 2에 나와 있습니다. 
<br/>
![](/assets/img/2023-08-30-14-09-29.png)
<br/>
![](/assets/img/2023-08-30-14-12-32.png)
<br/>

## Experiments

### image Classification 
4.2절에서 설명하는 디베이즈 대비 학습에 대한 관련 방법인 DCL [8] 및 HCL [36]을 사용하여 모델(WCL)을 평가합니다. 유명한 이미지 분류 데이터 세트에서 이 방법들을 비교합니다: CIFAR10, CIFAR-100 [23], CelebA [27], Waterbirds [37]. CIFAR-10과 CIFAR-100의 경우, 우리는 백본 인코더 구조로 ResNet50 [13]을 사용하고, 다른 모든 데이터 세트에는 ResNet18을 사용합니다. 우리의 방법에서 삼중 손실이 있는 바이어스드 인코더인 Eb도 동일한 최적화 체계로 동일한 구조를 가집니다. CelebA 데이터셋의 경우, 편향성 완화에서 이전 연구[32, 34]를 따라 매력도를 타겟 레이블로 고려합니다. 매력적인 남성은 다른 그룹에 비해 정확도가 가장 낮은 것으로 알려져 있습니다. 물새 데이터셋은 이미지 속 새가 물새인지 육지새인지 예측하는 데이터셋으로, 배경(물 또는 육지)이 편향을 만드는 것으로 알려져 있습니다. 
<br/>

#### Quantitative Results 
표 1에는 CIFAR10과 CIFAR-100에 대한 테스트 정확도 결과가 나와 있습니다. ACC(최악)는 최악의 클래스 정확도를 나타내며, 이는 대부분 편향성이 있는 것으로 간주됩니다. 여기에서는 분류 작업에서 편향성을 제거하는 것을 목표로 하는 JTT[26]를 포함했습니다. 다른 방법과 비교했을 때, 제안된 디베이즈드 인코더 Ed로 훈련된 분류기는 두 데이터 세트 모두에서 모든 정확도 측정값을 능가합니다. 흥미로운 점은 최악의 그룹 정확도뿐만 아니라 전반적인 정확도도 개선되었다는 점입니다. 이는 우리의 방법이 더 나은 일반화와 견고성을 가지고 있음을 보여줍니다. 편향 증폭 인코더를 통해 상대적 난이도를 활용한다는 주장을 검증하기 위해 편향 인코더 Eb로 분류기를 훈련할 때 정확도, 특히 ACC(최악)가 상당히 낮아졌습니다. ACC(상위 1)와 ACC(최악) 사이의 격차는 다른 방법보다 훨씬 큽니다. 예를 들어 CIFAR-100에서 삼중 손실인 Eb로 훈련된 인코더의 경우 최악의 클래스에 대한 정보를 거의 학습하지 못합니다. 이는 삼중 손실이 편향을 증폭시키고 모델이 쉬운 샘플에 더 집중하게 만든다는 우리의 주장을 뒷받침합니다.<br/>
![](/assets/img/2023-08-30-14-15-46.png)
<br/>
또한 편향된 이미지 벤치마크 데이터 세트에 대해 방법을 평가했습니다: 표 2의 CelebA와 Waterbirds입니다. 다른 대조 학습 방법과 비교했을 때, WCL은 최악의 그룹 정확도 측면에서 훨씬 뛰어난 성능을 보였습니다. 특히, WCL은 CelebA 데이터 세트에서 최첨단(HCL)보다 거의 6배 더 나은 결과를 얻을 수 있었습니다. 또한 다른 디베이싱 방법(DCL, HCL)은 최악의 그룹 정확도를 개선하기 위해 전체 정확도를 희생하는 것을 관찰했습니다. 반면, 우리는 전체 정확도를 유지하면서 최악의 그룹 정확도를 크게 개선할 수 있었습니다. 결과는 상대적 난이도 활용의 효과가 편향된 데이터에서 더 두드러진다는 것을 시사합니다. 또한, 결과는 대조 학습 접근법이 데이터 보강을 통해 학습된 표현의 일반성[4]을 활용함으로써 다중 클래스 시나리오에서 JTT보다 더 나은 일반화와 견고성을 달성한다는 것을 보여줍니다. ImageNet-100과 가중치 분석에 대한 추가 실험은 부록에서 확인할 수 있습니다.
<br/>

#### Qualitative Results
그림 3은 다양한 방법으로 학습된 표현의 t-SNE 시각화[40]를 보여줍니다. 점의 색은 CIFAR-10 데이터 세트에서 10개의 서로 다른 클래스를 나타냅니다. 난이도 기반 재가중화 방법(WCL, Ed)이 다른 방법보다 클래스 간 분리가 더 잘 이루어집니다. 또한 한 클래스 내의 샘플 분포가 우리 방식이 아닌 다른 방식에 대해 높은 분산을 보인다는 점도 흥미롭습니다. 이는 분포 간 중첩으로 이어져 편향된 예측을 초래할 수 있습니다. 이와는 대조적으로, 우리의 방법은 상대적으로 작은 분산을 나타내며, 이는 어려운 샘플을 잘 처리했다는 것을 의미합니다.<br/>
![](/assets/img/2023-08-30-14-17-20.png)
<br/>

### Sanity Check for Relative Distance
상대적 거리인 w(-)가 편향을 나타내는지 확인하기 위해 그림 4에서 무작위로 선택한 앵커로부터 상위 5개의 쉬운 네거티브와 어려운 네거티브를 표시합니다. 앵커(물 위의 물새)가 주어졌을 때, 이지 네거티브 샘플에서는 3개의 육지 위의 물새를 관찰하고 하드 네거티브 샘플에서는 2개의 육지 위의 물새를 관찰합니다. 이미지의 배경은 라벨과 스퓨리어스 상관관계[26]가 있기 때문에, 제안된 프레임워크가 스퓨리어스 상관관계를 올바르게 인식하고 Ed를 훈련하는 동안 불이익을 주는 것을 관찰할 수 있습니다.<br/>
![](/assets/img/2023-08-30-14-18-32.png)
<br/>

## Conclusion
우리는 데이터 편향의 관점에서 하드 네거티브 샘플을 강조하기 위한 새로운 대조 학습 전략을 제안합니다. 이는 편향된 표현 학습을 해결하기 위해 롤즈 최대-최소 공정성[35] 접근 방식을 취합니다. 우리는 편향 증폭 모델을 훈련하기 위해 삼중 손실을 제안하고 삼중 손실의 이론적 발견에 기반한 상대적 난이도를 얻기 위해 상대적 거리 대리자를 활용합니다. 이러한 상대적 난이도는 하드 네거티브 샘플링으로 이어져 비지도 방식으로 바이어스된 모델을 훈련할 수 있습니다.